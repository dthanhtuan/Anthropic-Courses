{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Retrieval for Large Documents\n",
    "\n",
    "When the original document is too large to add full context to every chunk, we need strategies to maintain information while keeping context manageable.\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Hierarchical Chunking** - Add parent section context instead of full document\n",
    "2. **Summary-Based Context** - Use document summaries\n",
    "3. **Metadata Extraction** - Add structured metadata\n",
    "4. **Sliding Window Context** - Include surrounding chunks\n",
    "\n",
    "We'll use the VoyageAI API Key Directions PDF as our example document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "load_dotenv()\n",
    "client = Anthropic()\n",
    "model = \"claude-3-5-sonnet-20241022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document from VoyageAI_API_Key_Directions.pdf\n",
    "VOYAGE_API_DOCUMENT = \"\"\"Getting an API Key from VoyageAI\n",
    "\n",
    "Step 1: Sign up to VoyageAI\n",
    "1. Navigate to https://www.voyageai.com/\n",
    "2. Click on the Login button in the top right corner\n",
    "3. Create an account\n",
    "\n",
    "Step 2: Create an API key\n",
    "1. Once logged in, find the 'API Keys' section on the left nav bar\n",
    "2. Click the 'Create new secret key button'\n",
    "3. Enter a key name of \"Test Key\" then click 'Create secret key'\n",
    "4. Copy the key\n",
    "\n",
    "Step 3: Add the key to your \".env\" file\n",
    "1. In your editor, find your \".env\" file that's next to your notebook.\n",
    "2. Add in your api key, assigning it to a variable named exactly \"VOYAGE_API_KEY\"\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(VOYAGE_API_DOCUMENT)} characters\")\n",
    "print(f\"\\nDocument preview:\\n{VOYAGE_API_DOCUMENT[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: Hierarchical Chunking\n",
    "\n",
    "Instead of adding the entire document to each chunk, we extract the section/chapter title and add only that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hierarchical_chunks(document: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Split document into sections and create chunks with section context.\n",
    "    Each chunk only includes its immediate parent section, not the full document.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Split by major sections (Step 1, Step 2, etc.)\n",
    "    sections = re.split(r'(Step \\d+:[^\\n]+)', document)\n",
    "    \n",
    "    current_section = \"Introduction\"\n",
    "    \n",
    "    for i, part in enumerate(sections):\n",
    "        if not part.strip():\n",
    "            continue\n",
    "            \n",
    "        # Check if this is a section header\n",
    "        if part.startswith('Step '):\n",
    "            current_section = part.strip()\n",
    "        else:\n",
    "            # This is content - split into smaller chunks if needed\n",
    "            content_parts = part.strip().split('\\n\\n')\n",
    "            \n",
    "            for content in content_parts:\n",
    "                if content.strip():\n",
    "                    # Add section context to chunk\n",
    "                    enriched_chunk = {\n",
    "                        'section': current_section,\n",
    "                        'original_content': content.strip(),\n",
    "                        'contextualized_content': f\"Section: {current_section}\\n\\nContent: {content.strip()}\"\n",
    "                    }\n",
    "                    chunks.append(enriched_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "hierarchical_chunks = extract_hierarchical_chunks(VOYAGE_API_DOCUMENT)\n",
    "\n",
    "print(f\"Created {len(hierarchical_chunks)} hierarchical chunks\\n\")\n",
    "for i, chunk in enumerate(hierarchical_chunks[:3], 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(f\"Section: {chunk['section']}\")\n",
    "    print(f\"Contextualized content:\\n{chunk['contextualized_content'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Summary-Based Context\n",
    "\n",
    "Generate a concise summary of the entire document and prepend it to each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_summary(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Claude to generate a concise summary of the document.\n",
    "    \"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=200,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Provide a 1-2 sentence summary of this document:\n",
    "\n",
    "{document}\n",
    "\n",
    "Keep it brief and focused on the main purpose.\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text\n",
    "\n",
    "doc_summary = generate_document_summary(VOYAGE_API_DOCUMENT)\n",
    "print(f\"Document Summary:\\n{doc_summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_summary_context(chunks: List[str], summary: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Add document summary to each chunk as context.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'original_content': chunk,\n",
    "            'contextualized_content': f\"Document Summary: {summary}\\n\\nChunk Content: {chunk}\"\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "# Simple chunking by paragraphs\n",
    "simple_chunks = [chunk.strip() for chunk in VOYAGE_API_DOCUMENT.split('\\n\\n') if chunk.strip()]\n",
    "summary_based_chunks = add_summary_context(simple_chunks[:3], doc_summary)\n",
    "\n",
    "print(f\"Created {len(summary_based_chunks)} summary-based chunks\\n\")\n",
    "print(\"Example chunk with summary context:\")\n",
    "print(summary_based_chunks[0]['contextualized_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Metadata Extraction\n",
    "\n",
    "Extract structured metadata using Claude and add it to chunks instead of raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_metadata(document: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract structured metadata from the document.\n",
    "    \"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=500,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Extract metadata from this document in JSON format:\n",
    "\n",
    "{document}\n",
    "\n",
    "Provide:\n",
    "- document_title: The main title/topic\n",
    "- document_type: Type of document (guide, tutorial, etc.)\n",
    "- key_topics: List of main topics covered\n",
    "- target_audience: Who this is for\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    text = response.content[0].text\n",
    "    # Try to find JSON in the response\n",
    "    json_match = re.search(r'\\{[^}]+\\}', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json.loads(json_match.group())\n",
    "    return json.loads(text)\n",
    "\n",
    "metadata = extract_document_metadata(VOYAGE_API_DOCUMENT)\n",
    "print(\"Extracted Metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata_context(chunks: List[str], metadata: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Add compact metadata to each chunk.\n",
    "    \"\"\"\n",
    "    metadata_header = f\"\"\"Document: {metadata.get('document_title', 'Unknown')}\n",
    "Type: {metadata.get('document_type', 'Unknown')}\n",
    "Topics: {', '.join(metadata.get('key_topics', []))}\n",
    "\"\"\"\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            'original_content': chunk,\n",
    "            'metadata': metadata,\n",
    "            'contextualized_content': f\"{metadata_header}\\nContent: {chunk}\"\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "metadata_chunks = add_metadata_context(simple_chunks[:3], metadata)\n",
    "\n",
    "print(\"Example chunk with metadata context:\")\n",
    "print(metadata_chunks[1]['contextualized_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 4: Sliding Window Context\n",
    "\n",
    "Add context from surrounding chunks instead of the full document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sliding_window_context(chunks: List[str], window_size: int = 1) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Add context from previous and next chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of chunk strings\n",
    "        window_size: Number of chunks before/after to include\n",
    "    \"\"\"\n",
    "    enriched_chunks = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        context_parts = []\n",
    "        \n",
    "        # Add previous chunks (abbreviated)\n",
    "        for j in range(max(0, i - window_size), i):\n",
    "            preview = chunks[j][:100].replace('\\n', ' ')\n",
    "            context_parts.append(f\"[Previous chunk]: {preview}...\")\n",
    "        \n",
    "        # Add next chunks (abbreviated)\n",
    "        for j in range(i + 1, min(len(chunks), i + window_size + 1)):\n",
    "            preview = chunks[j][:100].replace('\\n', ' ')\n",
    "            context_parts.append(f\"[Next chunk]: {preview}...\")\n",
    "        \n",
    "        context_header = \"\\n\".join(context_parts)\n",
    "        \n",
    "        enriched_chunk = {\n",
    "            'chunk_index': i,\n",
    "            'original_content': chunk,\n",
    "            'contextualized_content': f\"{context_header}\\n\\n[Current chunk]:\\n{chunk}\" if context_parts else chunk\n",
    "        }\n",
    "        enriched_chunks.append(enriched_chunk)\n",
    "    \n",
    "    return enriched_chunks\n",
    "\n",
    "sliding_window_chunks = add_sliding_window_context(simple_chunks, window_size=1)\n",
    "\n",
    "print(f\"Created {len(sliding_window_chunks)} chunks with sliding window context\\n\")\n",
    "print(\"Example chunk (middle of document):\")\n",
    "print(sliding_window_chunks[3]['contextualized_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 5: Hybrid Approach\n",
    "\n",
    "Combine multiple strategies for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_contextual_chunks(document: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combine hierarchical chunking + metadata + summary for best results.\n",
    "    \"\"\"\n",
    "    # 1. Extract metadata (compact, structured info)\n",
    "    metadata = extract_document_metadata(document)\n",
    "    \n",
    "    # 2. Generate brief summary\n",
    "    summary = generate_document_summary(document)\n",
    "    \n",
    "    # 3. Create hierarchical chunks\n",
    "    hierarchical_chunks = extract_hierarchical_chunks(document)\n",
    "    \n",
    "    # 4. Enrich each chunk with combined context\n",
    "    hybrid_chunks = []\n",
    "    for chunk_data in hierarchical_chunks:\n",
    "        # Create compact header\n",
    "        header = f\"\"\"[Document: {metadata.get('document_title', 'Unknown')}]\n",
    "[Summary: {summary}]\n",
    "[Section: {chunk_data['section']}]\n",
    "\"\"\"\n",
    "        \n",
    "        hybrid_chunk = {\n",
    "            'metadata': metadata,\n",
    "            'summary': summary,\n",
    "            'section': chunk_data['section'],\n",
    "            'original_content': chunk_data['original_content'],\n",
    "            'contextualized_content': f\"{header}\\n{chunk_data['original_content']}\"\n",
    "        }\n",
    "        hybrid_chunks.append(hybrid_chunk)\n",
    "    \n",
    "    return hybrid_chunks\n",
    "\n",
    "hybrid_chunks = create_hybrid_contextual_chunks(VOYAGE_API_DOCUMENT)\n",
    "\n",
    "print(f\"Created {len(hybrid_chunks)} hybrid contextual chunks\\n\")\n",
    "print(\"Example hybrid chunk:\")\n",
    "print(hybrid_chunks[2]['contextualized_content'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nOriginal content (for comparison):\")\n",
    "print(hybrid_chunks[2]['original_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Context Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def analyze_chunk_sizes(chunks: List[Dict], label: str):\n",
    "    \"\"\"\n",
    "    Analyze the overhead added by contextualization.\n",
    "    \"\"\"\n",
    "    original_sizes = [len(c['original_content']) for c in chunks]\n",
    "    contextualized_sizes = [len(c['contextualized_content']) for c in chunks]\n",
    "    overhead = [ctx - orig for orig, ctx in zip(original_sizes, contextualized_sizes)]\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Average original size: {statistics.mean(original_sizes):.0f} chars\")\n",
    "    print(f\"  Average contextualized size: {statistics.mean(contextualized_sizes):.0f} chars\")\n",
    "    print(f\"  Average overhead: {statistics.mean(overhead):.0f} chars ({statistics.mean(overhead)/statistics.mean(original_sizes)*100:.1f}%)\")\n",
    "    print(f\"  Total chunks: {len(chunks)}\")\n",
    "\n",
    "# Compare all strategies\n",
    "analyze_chunk_sizes(hierarchical_chunks, \"Hierarchical Chunking\")\n",
    "analyze_chunk_sizes(summary_based_chunks, \"Summary-Based Context\")\n",
    "analyze_chunk_sizes(metadata_chunks, \"Metadata Context\")\n",
    "analyze_chunk_sizes(sliding_window_chunks, \"Sliding Window\")\n",
    "analyze_chunk_sizes(hybrid_chunks, \"Hybrid Approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with RAG Query\n",
    "\n",
    "Let's simulate a simple retrieval scenario to see how contextual information helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_keyword_search(query: str, chunks: List[Dict], top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Simple keyword-based search (without embeddings).\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Score chunks by keyword overlap\n",
    "    scored_chunks = []\n",
    "    for chunk in chunks:\n",
    "        content_lower = chunk['original_content'].lower()\n",
    "        score = sum(1 for word in query_lower.split() if word in content_lower)\n",
    "        scored_chunks.append((score, chunk))\n",
    "    \n",
    "    # Sort by score and return top k\n",
    "    scored_chunks.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [chunk for score, chunk in scored_chunks[:top_k]]\n",
    "\n",
    "# Test query\n",
    "test_query = \"How do I create a new API key?\"\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare retrieval with different strategies\n",
    "print(\"\\n1. WITHOUT CONTEXT (original chunks):\")\n",
    "results = simple_keyword_search(test_query, hierarchical_chunks, top_k=1)\n",
    "print(results[0]['original_content'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n2. WITH HYBRID CONTEXT:\")\n",
    "print(results[0]['contextualized_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Contextualized Chunks with Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(query: str, chunks: List[Dict], use_context: bool = True):\n",
    "    \"\"\"\n",
    "    Answer a query using retrieved chunks.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved = simple_keyword_search(query, chunks, top_k=2)\n",
    "    \n",
    "    # Build context from chunks\n",
    "    if use_context:\n",
    "        context = \"\\n\\n\".join([c['contextualized_content'] for c in retrieved])\n",
    "    else:\n",
    "        context = \"\\n\\n\".join([c['original_content'] for c in retrieved])\n",
    "    \n",
    "    # Query Claude\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=500,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text\n",
    "\n",
    "# Test both approaches\n",
    "print(\"Query:\", test_query)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nAnswer WITHOUT contextual information:\")\n",
    "print(answer_with_rag(test_query, hybrid_chunks, use_context=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nAnswer WITH contextual information:\")\n",
    "print(answer_with_rag(test_query, hybrid_chunks, use_context=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "When dealing with large documents:\n",
    "\n",
    "1. **Hierarchical Chunking** - Best for structured documents (manuals, guides)\n",
    "   - Adds ~50-100 chars per chunk\n",
    "   - Preserves document structure\n",
    "\n",
    "2. **Summary-Based Context** - Best for consistent global context\n",
    "   - Adds ~100-200 chars per chunk\n",
    "   - Works well for thematic documents\n",
    "\n",
    "3. **Metadata Context** - Best for filtering and categorization\n",
    "   - Minimal overhead (~50-100 chars)\n",
    "   - Enables faceted search\n",
    "\n",
    "4. **Sliding Window** - Best for sequential/narrative content\n",
    "   - Variable overhead (depends on window size)\n",
    "   - Maintains flow\n",
    "\n",
    "5. **Hybrid Approach** - Best overall performance\n",
    "   - Combines benefits of multiple strategies\n",
    "   - ~200-300 chars overhead but significantly better context\n",
    "\n",
    "**Recommendation**: Use the hybrid approach for most production systems, adjusting based on:\n",
    "- Document size and structure\n",
    "- Query patterns\n",
    "- Token budget constraints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


{
 "cells": [
  {
   "cell_type": "code",
   "id": "5437be1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:34.676115510Z",
     "start_time": "2026-02-03T10:09:34.650543252Z"
    }
   },
   "source": [
    "# Load env variables and create client\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-haiku-4-5\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3b0d8e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:34.700680326Z",
     "start_time": "2026-02-03T10:09:34.677381350Z"
    }
   },
   "source": [
    "# Helper functions\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1e788701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:34.760044626Z",
     "start_time": "2026-02-03T10:09:34.727981150Z"
    }
   },
   "source": [
    "# Function to generate a new dataset\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "438ed743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:36.954198388Z",
     "start_time": "2026-02-03T10:09:34.762747382Z"
    }
   },
   "source": [
    "# Generate the dataset and write it to 'dataset.json'\n",
    "dataset = generate_dataset()\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "36b89174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:37.065479455Z",
     "start_time": "2026-02-03T10:09:36.995440428Z"
    }
   },
   "source": [
    "# Function to grade a test case + output using a model\n",
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "83809a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:37.099225174Z",
     "start_time": "2026-02-03T10:09:37.073799729Z"
    }
   },
   "source": [
    "# Passes a test case into Claude\n",
    "def run_prompt(test_case):\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with Python, JSON, or a plain Regex\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "7953c666",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:37.130012543Z",
     "start_time": "2026-02-03T10:09:37.101336867Z"
    }
   },
   "source": [
    "# Functions to validate the output structure\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "9bcc4671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:37.165584003Z",
     "start_time": "2026-02-03T10:09:37.131911363Z"
    }
   },
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "5fa99d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:09:37.188731343Z",
     "start_time": "2026-02-03T10:09:37.167247870Z"
    }
   },
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "30fae983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:10:11.954137257Z",
     "start_time": "2026-02-03T10:09:37.189818844Z"
    }
   },
   "source": [
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 6.666666666666667\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "dbcc6111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:10:12.008529048Z",
     "start_time": "2026-02-03T10:10:11.970907995Z"
    }
   },
   "source": [
    "print(json.dumps(results, indent=2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\nimport re\\nimport json\\n\\ndef parse_cloudwatch_log(log_entry):\\n    # AWS CloudWatch log format: YYYY-MM-DD HH:MM:SS.mmm [LEVEL] message\\n    pattern = r'^(\\\\d{4}-\\\\d{2}-\\\\d{2}\\\\s\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3})\\\\s\\\\[(\\\\w+)\\\\]\\\\s(.+)$'\\n    \\n    match = re.match(pattern, log_entry)\\n    \\n    if match:\\n        return {\\n            \\\"timestamp\\\": match.group(1),\\n            \\\"log_level\\\": match.group(2),\\n            \\\"message\\\": match.group(3)\\n        }\\n    return None\\n\\n# Test with example\\ntest_log = \\\"2024-01-15 10:30:45.123 [ERROR] Database connection failed\\\"\\nresult = parse_cloudwatch_log(test_log)\\nprint(json.dumps(result, indent=2))\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Parse an AWS CloudWatch log entry and extract the timestamp, log level, and message using regex\",\n",
      "      \"format\": \"regex\"\n",
      "    },\n",
      "    \"score\": 8.0,\n",
      "    \"reasoning\": \"The solution correctly solves the stated task with a working regex pattern and clean implementation. However, it lacks robustness for production use: it assumes a single fixed log format without validation, doesn't handle edge cases or malformed input gracefully, and lacks comprehensive documentation. For real-world AWS CloudWatch parsing, logs can have multiple formats (JSON structured logs, different timestamp formats, etc.), and the code should be more defensive.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport re\\n\\ndef is_valid_s3_bucket_name(bucket_name: str) -> bool:\\n    \\\"\\\"\\\"\\n    Validates if a given string is a valid AWS S3 bucket name.\\n    \\n    AWS S3 bucket naming rules:\\n    - Must be between 3 and 63 characters long\\n    - Can only contain lowercase letters, numbers, hyphens, and periods\\n    - Must start and end with a letter or number\\n    - Cannot contain consecutive periods\\n    - Cannot be formatted as an IP address (e.g., 192.168.1.1)\\n    \\\"\\\"\\\"\\n    if not isinstance(bucket_name, str):\\n        return False\\n    \\n    if len(bucket_name) < 3 or len(bucket_name) > 63:\\n        return False\\n    \\n    if not re.match(r'^[a-z0-9][a-z0-9\\\\.\\\\-]*[a-z0-9]$', bucket_name) and not re.match(r'^[a-z0-9]$', bucket_name):\\n        return False\\n    \\n    if '..' in bucket_name:\\n        return False\\n    \\n    if re.match(r'^(\\\\d+\\\\.){3}\\\\d+$', bucket_name):\\n        return False\\n    \\n    return True\\n\\n\\n# Test cases\\ntest_cases = [\\n    (\\\"my-bucket\\\", True),\\n    (\\\"my.bucket\\\", True),\\n    (\\\"mybucket\\\", True),\\n    (\\\"my-bucket-123\\\", True),\\n    (\\\"a\\\", False),\\n    (\\\"ab\\\", False),\\n    (\\\"abc\\\", True),\\n    (\\\"my--bucket\\\", True),\\n    (\\\"my..bucket\\\", False),\\n    (\\\"-mybucket\\\", False),\\n    (\\\"mybucket-\\\", False),\\n    (\\\"MyBucket\\\", False),\\n    (\\\"my_bucket\\\", False),\\n    (\\\"192.168.1.1\\\", False),\\n    (\\\"my-bucket-\\\", False),\\n    (\\\"a\\\" * 64, False),\\n    (\\\"a\\\" * 63, True),\\n]\\n\\nfor bucket_name, expected in test_cases:\\n    result = is_valid_s3_bucket_name(bucket_name)\\n    status = \\\"\\u2713\\\" if result == expected else \\\"\\u2717\\\"\\n    print(f\\\"{status} {bucket_name}: {result} (expected {expected})\\\")\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a Python function that validates if a given string is a valid AWS S3 bucket name\",\n",
      "      \"format\": \"python\"\n",
      "    },\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The solution demonstrates good understanding of the problem with solid test coverage and documentation. However, there is a critical flaw: the regex validation doesn't properly enforce AWS's rule that bucket names cannot start or end with periods. Testing 'my.bucket' passes, but 'mybucket.' should fail (and does in the regex, but only because of the end anchor requirement for alphanumeric). The implementation would incorrectly accept '.mybucket' as valid. Additionally, the regex pattern could be streamlined using a character class more efficiently. The logic is mostly correct for the tested cases, but incomplete for all edge cases.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport json\\n\\npolicy = {\\n    \\\"Version\\\": \\\"2012-10-17\\\",\\n    \\\"Statement\\\": [\\n        {\\n            \\\"Effect\\\": \\\"Allow\\\",\\n            \\\"Action\\\": [\\n                \\\"s3:GetObject\\\",\\n                \\\"s3:GetObjectVersion\\\",\\n                \\\"s3:ListBucket\\\",\\n                \\\"s3:GetBucketVersioning\\\",\\n                \\\"s3:GetBucketLocation\\\"\\n            ],\\n            \\\"Resource\\\": [\\n                \\\"arn:aws:s3:::your-bucket-name\\\",\\n                \\\"arn:aws:s3:::your-bucket-name/*\\\"\\n            ]\\n        }\\n    ]\\n}\\n\\nprint(json.dumps(policy, indent=2))\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Generate a JSON object representing an AWS IAM policy that allows read-only access to a specific S3 bucket\",\n",
      "      \"format\": \"json\"\n",
      "    },\n",
      "    \"score\": 3.5,\n",
      "    \"reasoning\": \"The solution demonstrates solid understanding of IAM policy structure and S3 permissions. The policy correctly allows read-only operations with proper resource targeting. However, it could be more refined by removing non-essential permissions (GetBucketVersioning) to strictly adhere to least privilege, and could benefit from clearer documentation about the placeholder value and use case. The core implementation is sound and would function correctly once the bucket name is specified.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

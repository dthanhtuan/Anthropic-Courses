{
 "cells": [
  {
   "cell_type": "code",
   "id": "5437be1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:47.206595772Z",
     "start_time": "2026-02-03T10:32:47.181301286Z"
    }
   },
   "source": [
    "# Load env variables and create client\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-haiku-4-5\""
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "3b0d8e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:47.232955941Z",
     "start_time": "2026-02-03T10:32:47.208202405Z"
    }
   },
   "source": [
    "# Helper functions\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "1e788701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:47.272357340Z",
     "start_time": "2026-02-03T10:32:47.234045467Z"
    }
   },
   "source": [
    "# Function to generate a new dataset\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\",\n",
    "        \"solution_criteria\": \"Key criteria for evaluating the solution\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "438ed743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:50.075969794Z",
     "start_time": "2026-02-03T10:32:47.275466839Z"
    }
   },
   "source": [
    "# Generate the dataset and write it to 'dataset.json'\n",
    "dataset = generate_dataset()\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "36b89174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:50.107236204Z",
     "start_time": "2026-02-03T10:32:50.083726993Z"
    }
   },
   "source": [
    "# Function to grade a test case + output using a model\n",
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Criteria you should use to evaluate the solution:\n",
    "<criteria>\n",
    "{test_case[\"solution_criteria\"]}\n",
    "</criteria>\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "83809a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:50.123991887Z",
     "start_time": "2026-02-03T10:32:50.108391606Z"
    }
   },
   "source": [
    "# Passes a test case into Claude\n",
    "def run_prompt(test_case):\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with Python, JSON, or a plain Regex\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7953c666",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:50.149051072Z",
     "start_time": "2026-02-03T10:32:50.125086849Z"
    }
   },
   "source": [
    "# Functions to validate the output structure\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "9bcc4671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:50.162765475Z",
     "start_time": "2026-02-03T10:32:50.150044879Z"
    }
   },
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "5fa99d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:32:50.214402626Z",
     "start_time": "2026-02-03T10:32:50.187786926Z"
    }
   },
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "30fae983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:33:08.334884010Z",
     "start_time": "2026-02-03T10:32:50.215549563Z"
    }
   },
   "source": [
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 8.0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "dbcc6111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T10:33:08.388862028Z",
     "start_time": "2026-02-03T10:33:08.343945429Z"
    }
   },
   "source": [
    "print(json.dumps(results, indent=2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\nimport json\\nimport re\\nfrom collections import defaultdict\\n\\ndef parse_cloudwatch_logs_insights(query_result):\\n    \\\"\\\"\\\"\\n    Parse AWS CloudWatch Logs Insights query result and extract count of errors by service name.\\n    \\n    Args:\\n        query_result: The query result from CloudWatch Logs Insights (typically a list of records)\\n    \\n    Returns:\\n        Dictionary with service names as keys and error counts as values\\n    \\\"\\\"\\\"\\n    error_counts = defaultdict(int)\\n    \\n    if isinstance(query_result, str):\\n        query_result = json.loads(query_result)\\n    \\n    if isinstance(query_result, list):\\n        for record in query_result:\\n            if isinstance(record, dict):\\n                service_name = record.get('serviceName') or record.get('service_name') or record.get('service')\\n                \\n                message = record.get('message', '') or record.get('@message', '')\\n                level = record.get('level') or record.get('@level') or record.get('severity')\\n                \\n                is_error = (\\n                    'error' in str(level).lower() or \\n                    'error' in str(message).lower() or\\n                    'exception' in str(message).lower() or\\n                    'failed' in str(message).lower()\\n                )\\n                \\n                if service_name and is_error:\\n                    error_counts[service_name] += 1\\n    \\n    return dict(error_counts)\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Parse an AWS CloudWatch Logs Insights query result and extract the count of errors by service name\",\n",
      "      \"format\": \"python\",\n",
      "      \"solution_criteria\": \"Function should accept a list of log records, filter for error-level entries, group by service name field, and return a dictionary with service names as keys and error counts as values\"\n",
      "    },\n",
      "    \"score\": 8.0,\n",
      "    \"reasoning\": \"The solution meets the basic criteria of accepting log records, filtering by error indicators, grouping by service name, and returning the expected dictionary format. However, the error detection mechanism is overly simplistic and prone to false positives (e.g., 'The error-free service' would match). A production solution should prioritize checking actual log level fields first before falling back to message content. Additionally, CloudWatch Logs Insights queries often return pre-aggregated results with fields like '@count' or custom metrics, which this solution doesn't handle. The lack of input validation also leaves it vulnerable to crashes with unexpected data.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"AWSTemplateFormatVersion\\\": \\\"2010-09-09\\\",\\n  \\\"Resources\\\": {\\n    \\\"S3Bucket\\\": {\\n      \\\"Type\\\": \\\"AWS::S3::Bucket\\\",\\n      \\\"Properties\\\": {\\n        \\\"BucketName\\\": \\\"my-secure-bucket\\\",\\n        \\\"VersioningConfiguration\\\": {\\n          \\\"Status\\\": \\\"Enabled\\\"\\n        },\\n        \\\"PublicAccessBlockConfiguration\\\": {\\n          \\\"BlockPublicAcls\\\": true,\\n          \\\"BlockPublicPolicy\\\": true,\\n          \\\"IgnorePublicAcls\\\": true,\\n          \\\"RestrictPublicBuckets\\\": true\\n        }\\n      }\\n    }\\n  },\\n  \\\"Outputs\\\": {\\n    \\\"BucketName\\\": {\\n      \\\"Value\\\": {\\n        \\\"Ref\\\": \\\"S3Bucket\\\"\\n      },\\n      \\\"Description\\\": \\\"Name of the S3 bucket\\\"\\n    },\\n    \\\"BucketArn\\\": {\\n      \\\"Value\\\": {\\n        \\\"Fn::GetAtt\\\": [\\n          \\\"S3Bucket\\\",\\n          \\\"Arn\\\"\\n        ]\\n      },\\n      \\\"Description\\\": \\\"ARN of the S3 bucket\\\"\\n    }\\n  }\\n}\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a CloudFormation template snippet that defines an S3 bucket with versioning enabled and public access blocked\",\n",
      "      \"format\": \"json\",\n",
      "      \"solution_criteria\": \"Valid JSON that includes S3 bucket resource with VersioningConfiguration and PublicAccessBlockConfiguration properties properly configured\"\n",
      "    },\n",
      "    \"score\": 9.0,\n",
      "    \"reasoning\": \"The solution meets the core requirements with valid JSON and proper configuration of versioning and public access blocking. The CloudFormation syntax is correct and the security settings are comprehensive. However, the hard-coded bucket name is a practical deployment issue, and the template could be more production-ready with additional security configurations like encryption. Despite these minor gaps, the solution correctly addresses the stated task requirements.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport re\\n\\ndef extract_iam_role_arns(text):\\n    pattern = r'arn:aws:iam::\\\\d{12}:role/[a-zA-Z0-9\\\\-_/.@]+'\\n    matches = re.findall(pattern, text)\\n    return matches\\n\\n# Test\\ntest_text = \\\"\\\"\\\"\\narn:aws:iam::123456789012:role/MyRole\\narn:aws:iam::123456789012:role/service-role/MyServiceRole\\narn:aws:iam::987654321098:role/Lambda-Execution-Role\\ninvalid:arn:iam::123456789012:role/BadRole\\narn:aws:iam::123456789012:role/path/to/my-role\\n\\\"\\\"\\\"\\n\\nresult = extract_iam_role_arns(test_text)\\nfor arn in result:\\n    print(arn)\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a regex pattern to validate and extract AWS IAM role ARNs from a block of text\",\n",
      "      \"format\": \"regex\",\n",
      "      \"solution_criteria\": \"Pattern should match valid IAM role ARNs in the format 'arn:aws:iam::account-id:role/role-name', capture the account ID and role name separately, and reject invalid formats\"\n",
      "    },\n",
      "    \"score\": 7.0,\n",
      "    \"reasoning\": \"The solution partially addresses the task by validating and extracting ARNs, but it fundamentally fails to meet a key requirement: separating and capturing account ID and role name as distinct components. The regex pattern is functional but oversimplified, allowing invalid character combinations and not providing structured extraction. A compliant solution should use capture groups like: r'arn:aws:iam::([0-9]{12}):role/([a-zA-Z0-9+=,.@\\\\-_/]+)' and return tuples of (account_id, role_name).\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
